{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/spaidermen/BLHeli/blob/master/examples/timeseries/ipynb/timeseries_classification_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVbW2dfd0zVf"
      },
      "source": [
        "# Timeseries classification from scratch\n",
        "\n",
        "**Author:** [hfawaz](https://github.com/hfawaz/)<br>\n",
        "**Date created:** 2020/07/21<br>\n",
        "**Last modified:** 2023/11/10<br>\n",
        "**Description:** Training a timeseries classifier from scratch on the FordA dataset from the UCR/UEA archive."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZFx7Zr00zVj"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "This example shows how to do timeseries classification from scratch, starting from raw\n",
        "CSV timeseries files on disk. We demonstrate the workflow on the FordA dataset from the\n",
        "[UCR/UEA archive](https://www.cs.ucr.edu/%7Eeamonn/time_series_data_2018/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4Ei8blr0zVk"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jstBcBAN0zVk"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJ51gR4x0zVm"
      },
      "source": [
        "## Load the data: the FordA dataset\n",
        "\n",
        "### Dataset description\n",
        "\n",
        "The dataset we are using here is called FordA.\n",
        "The data comes from the UCR archive.\n",
        "The dataset contains 3601 training instances and another 1320 testing instances.\n",
        "Each timeseries corresponds to a measurement of engine noise captured by a motor sensor.\n",
        "For this task, the goal is to automatically detect the presence of a specific issue with\n",
        "the engine. The problem is a balanced binary classification task. The full description of\n",
        "this dataset can be found [here](http://www.j-wichard.de/publications/FordPaper.pdf).\n",
        "\n",
        "### Read the TSV data\n",
        "\n",
        "We will use the `FordA_TRAIN` file for training and the\n",
        "`FordA_TEST` file for testing. The simplicity of this dataset\n",
        "allows us to demonstrate effectively how to use ConvNets for timeseries classification.\n",
        "In this file, the first column corresponds to the label."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u5AW-3Ca0zVm"
      },
      "outputs": [],
      "source": [
        "\n",
        "def readucr(filename):\n",
        "    data = np.loadtxt(filename, delimiter=\"\\t\")\n",
        "    y = data[:, 0]\n",
        "    x = data[:, 1:]\n",
        "    return x, y.astype(int)\n",
        "\n",
        "\n",
        "root_url = \"https://raw.githubusercontent.com/hfawaz/cd-diagram/master/FordA/\"\n",
        "\n",
        "x_train, y_train = readucr(root_url + \"FordA_TRAIN.tsv\")\n",
        "x_test, y_test = readucr(root_url + \"FordA_TEST.tsv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7puwdR20zVn"
      },
      "source": [
        "## Visualize the data\n",
        "\n",
        "Here we visualize one timeseries example for each class in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Cjkop6P70zVn",
        "outputId": "cbeca22b-aabc-4b06-cc4e-75c4620acca8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'np' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-4ca276e6cd87>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mc_x_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my_train\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
          ]
        }
      ],
      "source": [
        "classes = np.unique(np.concatenate((y_train, y_test), axis=0))\n",
        "\n",
        "plt.figure()\n",
        "for c in classes:\n",
        "    c_x_train = x_train[y_train == c]\n",
        "    plt.plot(c_x_train[0], label=\"class \" + str(c))\n",
        "plt.legend(loc=\"best\")\n",
        "plt.show()\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5yH8s0Y0zVo"
      },
      "source": [
        "## Standardize the data\n",
        "\n",
        "Our timeseries are already in a single length (500). However, their values are\n",
        "usually in various ranges. This is not ideal for a neural network;\n",
        "in general we should seek to make the input values normalized.\n",
        "For this specific dataset, the data is already z-normalized: each timeseries sample\n",
        "has a mean equal to zero and a standard deviation equal to one. This type of\n",
        "normalization is very common for timeseries classification problems, see\n",
        "[Bagnall et al. (2016)](https://link.springer.com/article/10.1007/s10618-016-0483-9).\n",
        "\n",
        "Note that the timeseries data used here are univariate, meaning we only have one channel\n",
        "per timeseries example.\n",
        "We will therefore transform the timeseries into a multivariate one with one channel\n",
        "using a simple reshaping via numpy.\n",
        "This will allow us to construct a model that is easily applicable to multivariate time\n",
        "series."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HOFHx8S50zVo"
      },
      "outputs": [],
      "source": [
        "x_train = x_train.reshape((x_train.shape[0], x_train.shape[1], 1))\n",
        "x_test = x_test.reshape((x_test.shape[0], x_test.shape[1], 1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zAtwsdJx0zVp"
      },
      "source": [
        "Finally, in order to use `sparse_categorical_crossentropy`, we will have to count\n",
        "the number of classes beforehand."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CjHh7SZQ0zVp"
      },
      "outputs": [],
      "source": [
        "num_classes = len(np.unique(y_train))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6PFcW890zVp"
      },
      "source": [
        "Now we shuffle the training set because we will be using the `validation_split` option\n",
        "later when training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Idd_eV-W0zVq"
      },
      "outputs": [],
      "source": [
        "idx = np.random.permutation(len(x_train))\n",
        "x_train = x_train[idx]\n",
        "y_train = y_train[idx]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZ2kgUDs0zVq"
      },
      "source": [
        "Standardize the labels to positive integers.\n",
        "The expected labels will then be 0 and 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0zKRYLH70zVq"
      },
      "outputs": [],
      "source": [
        "y_train[y_train == -1] = 0\n",
        "y_test[y_test == -1] = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ke6cvYVe0zVq"
      },
      "source": [
        "## Build a model\n",
        "\n",
        "We build a Fully Convolutional Neural Network originally proposed in\n",
        "[this paper](https://arxiv.org/abs/1611.06455).\n",
        "The implementation is based on the TF 2 version provided\n",
        "[here](https://github.com/hfawaz/dl-4-tsc/).\n",
        "The following hyperparameters (kernel_size, filters, the usage of BatchNorm) were found\n",
        "via random search using [KerasTuner](https://github.com/keras-team/keras-tuner)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GrWCDksK0zVq"
      },
      "outputs": [],
      "source": [
        "\n",
        "def make_model(input_shape):\n",
        "    input_layer = keras.layers.Input(input_shape)\n",
        "\n",
        "    conv1 = keras.layers.Conv1D(filters=64, kernel_size=3, padding=\"same\")(input_layer)\n",
        "    conv1 = keras.layers.BatchNormalization()(conv1)\n",
        "    conv1 = keras.layers.ReLU()(conv1)\n",
        "\n",
        "    conv2 = keras.layers.Conv1D(filters=64, kernel_size=3, padding=\"same\")(conv1)\n",
        "    conv2 = keras.layers.BatchNormalization()(conv2)\n",
        "    conv2 = keras.layers.ReLU()(conv2)\n",
        "\n",
        "    conv3 = keras.layers.Conv1D(filters=64, kernel_size=3, padding=\"same\")(conv2)\n",
        "    conv3 = keras.layers.BatchNormalization()(conv3)\n",
        "    conv3 = keras.layers.ReLU()(conv3)\n",
        "\n",
        "    gap = keras.layers.GlobalAveragePooling1D()(conv3)\n",
        "\n",
        "    output_layer = keras.layers.Dense(num_classes, activation=\"softmax\")(gap)\n",
        "\n",
        "    return keras.models.Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "\n",
        "model = make_model(input_shape=x_train.shape[1:])\n",
        "keras.utils.plot_model(model, show_shapes=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G94-EtpY0zVr"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jUDa8uu40zVr"
      },
      "outputs": [],
      "source": [
        "epochs = 500\n",
        "batch_size = 32\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\n",
        "        \"best_model.keras\", save_best_only=True, monitor=\"val_loss\"\n",
        "    ),\n",
        "    keras.callbacks.ReduceLROnPlateau(\n",
        "        monitor=\"val_loss\", factor=0.5, patience=20, min_lr=0.0001\n",
        "    ),\n",
        "    keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=50, verbose=1),\n",
        "]\n",
        "model.compile(\n",
        "    optimizer=\"adam\",\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    metrics=[\"sparse_categorical_accuracy\"],\n",
        ")\n",
        "history = model.fit(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs,\n",
        "    callbacks=callbacks,\n",
        "    validation_split=0.2,\n",
        "    verbose=1,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rirZQ0m0zVr"
      },
      "source": [
        "## Evaluate model on test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r3rKOkaJ0zVr"
      },
      "outputs": [],
      "source": [
        "model = keras.models.load_model(\"best_model.keras\")\n",
        "\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "\n",
        "print(\"Test accuracy\", test_acc)\n",
        "print(\"Test loss\", test_loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59Pz4enc0zVr"
      },
      "source": [
        "## Plot the model's training and validation loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bj6yxj_d0zVr"
      },
      "outputs": [],
      "source": [
        "metric = \"sparse_categorical_accuracy\"\n",
        "plt.figure()\n",
        "plt.plot(history.history[metric])\n",
        "plt.plot(history.history[\"val_\" + metric])\n",
        "plt.title(\"model \" + metric)\n",
        "plt.ylabel(metric, fontsize=\"large\")\n",
        "plt.xlabel(\"epoch\", fontsize=\"large\")\n",
        "plt.legend([\"train\", \"val\"], loc=\"best\")\n",
        "plt.show()\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpP3xZ8j0zVs"
      },
      "source": [
        "We can see how the training accuracy reaches almost 0.95 after 100 epochs.\n",
        "However, by observing the validation accuracy we can see how the network still needs\n",
        "training until it reaches almost 0.97 for both the validation and the training accuracy\n",
        "after 200 epochs. Beyond the 200th epoch, if we continue on training, the validation\n",
        "accuracy will start decreasing while the training accuracy will continue on increasing:\n",
        "the model starts overfitting."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "timeseries_classification_from_scratch",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}